import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from typing import Dict, List, Optional, Tuple
import logging
from tqdm import tqdm
import numpy as np
import dgl

from config.model_config import ModelConfig
from config.base_config import BaseConfig
from src.models.encoders.image_encoder import create_image_encoder
from src.models.encoders.brep_encoder import BrepEncoder
from src.models.fusion.multimodal_fusion import create_bimodal_fuser
from src.models.downstream.retrieval import (
    create_retrieval_model,
    create_retrieval_loss,
    RetrievalMetrics
)


class MultiModalRetrievalPipeline(nn.Module):
    """å¤šæ¨¡æ€æ£€ç´¢å®Œæ•´æµæ°´çº¿"""

    def __init__(self, model_config: ModelConfig):
        super().__init__()
        self.config = model_config

        # ç¼–ç å™¨
        self.image_encoder = create_image_encoder(model_config)
        self.brep_encoder = BrepEncoder(model_config)

        # èåˆæ¨¡å—
        self.fusion_module = create_bimodal_fuser(model_config)

        # æ£€ç´¢æ¨¡å—
        # ä½¿ç”¨é…ç½®ä¸­çš„è®¾ç½®å†³å®šæ˜¯å¦ä½¿ç”¨é«˜çº§æ£€ç´¢æ¨¡å‹
        use_advanced = getattr(model_config, 'use_advanced_retrieval', True)
        self.retrieval_module = create_retrieval_model(model_config, use_advanced=use_advanced)

        # æŸå¤±å‡½æ•°
        temperature = getattr(model_config, 'contrastive_temperature', 0.07)
        self.criterion = create_retrieval_loss(temperature=temperature, use_advanced=use_advanced)

    def encode_features(self, images: torch.Tensor,
                       brep_graphs: torch.Tensor) -> Dict[str, torch.Tensor]:
        """ç¼–ç è¾“å…¥ç‰¹å¾"""
        # å›¾åƒç¼–ç 
        image_features = self.image_encoder(images)

        # Brepç¼–ç 
        brep_output = self.brep_encoder(brep_graphs)
        brep_features = brep_output['features'] if isinstance(brep_output, dict) else brep_output

        # å¤šæ¨¡æ€èåˆ
        fusion_output = self.fusion_module(image_features, brep_features)

        return fusion_output

    def forward(self, query_images: torch.Tensor, query_brep: torch.Tensor,
                candidate_images: torch.Tensor, candidate_brep: torch.Tensor,
                labels: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­

        Args:
            query_images: æŸ¥è¯¢å›¾åƒ
            query_brep: æŸ¥è¯¢Brep
            candidate_images: å€™é€‰å›¾åƒ
            candidate_brep: å€™é€‰Brep
            labels: æ­£æ ·æœ¬æ ‡ç­¾

        Returns:
            è¾“å‡ºå­—å…¸
        """
        # ç¼–ç æŸ¥è¯¢ç‰¹å¾
        query_features = self.encode_features(query_images, query_brep)

        # ç¼–ç å€™é€‰ç‰¹å¾
        candidate_features = self.encode_features(candidate_images, candidate_brep)

        # æ£€ç´¢
        retrieval_output = self.retrieval_module(query_features, candidate_features, labels)

        return retrieval_output

    def retrieve(self, query_images: torch.Tensor, query_brep: torch.Tensor,
                candidate_images: torch.Tensor, candidate_brep: torch.Tensor,
                k: int = 10, modality: str = 'fused') -> Dict[str, torch.Tensor]:
        """
        æ‰§è¡Œæ£€ç´¢æ¨ç†

        Args:
            query_images: æŸ¥è¯¢å›¾åƒ
            query_brep: æŸ¥è¯¢Brep
            candidate_images: å€™é€‰å›¾åƒ
            candidate_brep: å€™é€‰Brep
            k: è¿”å›top-kç»“æœ
            modality: æ£€ç´¢æ¨¡æ€

        Returns:
            æ£€ç´¢ç»“æœ
        """
        with torch.no_grad():
            # ç¼–ç ç‰¹å¾
            query_features = self.encode_features(query_images, query_brep)
            candidate_features = self.encode_features(candidate_images, candidate_brep)

            # æ‰§è¡Œæ£€ç´¢
            results = self.retrieval_module.retrieve(
                query_features, candidate_features, k, modality
            )

            return results


class RetrievalTrainer:
    """æ£€ç´¢æ¨¡å‹è®­ç»ƒå™¨"""

    def __init__(self, model: MultiModalRetrievalPipeline,
                 model_config: ModelConfig, base_config: BaseConfig):
        self.model = model
        self.model_config = model_config
        self.base_config = base_config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        # ç§»åŠ¨æ¨¡å‹åˆ°è®¾å¤‡
        self.model = self.model.to(self.device)

        # ä¼˜åŒ–å™¨
        self.optimizer = self._create_optimizer()

        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = self._create_scheduler()

        # è¯„ä¼°æŒ‡æ ‡
        self.metrics = RetrievalMetrics()

        # æ—¥å¿—
        self.logger = logging.getLogger(__name__)

    def _create_optimizer(self) -> optim.Optimizer:
        """åˆ›å»ºä¼˜åŒ–å™¨"""
        optimizer_name = getattr(self.base_config, 'optimizer', 'adamw')

        if optimizer_name.lower() == 'adamw':
            return optim.AdamW(
                self.model.parameters(),
                lr=self.base_config.learning_rate,
                weight_decay=self.base_config.weight_decay
            )
        elif optimizer_name.lower() == 'adam':
            return optim.Adam(
                self.model.parameters(),
                lr=self.base_config.learning_rate,
                weight_decay=self.base_config.weight_decay
            )
        else:
            return optim.SGD(
                self.model.parameters(),
                lr=self.base_config.learning_rate,
                momentum=0.9,
                weight_decay=self.base_config.weight_decay
            )

    def _create_scheduler(self):
        """åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨"""
        scheduler_name = getattr(self.base_config, 'scheduler', 'cosine')

        if scheduler_name.lower() == 'cosine':
            return optim.lr_scheduler.CosineAnnealingLR(
                self.optimizer,
                T_max=self.base_config.num_epochs
            )
        elif scheduler_name.lower() == 'step':
            return optim.lr_scheduler.StepLR(
                self.optimizer,
                step_size=self.base_config.num_epochs // 3,
                gamma=0.1
            )
        else:
            return optim.lr_scheduler.ReduceLROnPlateau(
                self.optimizer,
                mode='max',
                factor=0.5,
                patience=5
            )

    def train_epoch(self, train_loader: DataLoader) -> Dict[str, float]:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0.0
        num_batches = 0

        progress_bar = tqdm(train_loader, desc="è®­ç»ƒ")

        for batch in progress_bar:
            # æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡
            images = batch['images'].to(self.device)
            brep_graphs = batch['brep_graph'].to(self.device)
            labels = batch['labels'].to(self.device)

            # æ„é€ å€™é€‰é›†ï¼ˆåœ¨è®­ç»ƒæ—¶ï¼Œä½¿ç”¨åŒä¸€æ‰¹æ¬¡çš„å…¶ä»–æ ·æœ¬ä½œä¸ºå€™é€‰ï¼‰
            batch_size = images.size(0)

            # ä¸ºæ¯ä¸ªæŸ¥è¯¢åˆ›å»ºå€™é€‰é›†ï¼ˆåŒ…å«æ­£æ ·æœ¬å’Œè´Ÿæ ·æœ¬ï¼‰
            candidate_images = images  # ä½¿ç”¨æ•´ä¸ªæ‰¹æ¬¡ä½œä¸ºå€™é€‰é›†
            candidate_brep = brep_graphs
            candidate_labels = labels

            # åˆ›å»ºæ­£æ ·æœ¬æ ‡ç­¾ï¼ˆæ¯ä¸ªæŸ¥è¯¢çš„æ­£æ ·æœ¬æ˜¯å®ƒè‡ªå·±åœ¨å€™é€‰é›†ä¸­çš„ä½ç½®ï¼‰
            positive_labels = torch.arange(batch_size, device=self.device)

            # å‰å‘ä¼ æ’­
            output = self.model(
                images, brep_graphs,
                candidate_images, candidate_brep,
                positive_labels
            )

            # è®¡ç®—æŸå¤±
            loss_output = self.model.criterion(output, positive_labels)
            loss = loss_output['total_loss']

            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()

            # ç»Ÿè®¡
            total_loss += loss.item()
            num_batches += 1

            # æ›´æ–°è¿›åº¦æ¡
            progress_bar.set_postfix({'Loss': loss.item()})

        # å­¦ä¹ ç‡è°ƒåº¦
        if isinstance(self.scheduler, optim.lr_scheduler.ReduceLROnPlateau):
            # å¯¹äºReduceLROnPlateauï¼Œéœ€è¦ä¼ å…¥æŒ‡æ ‡
            pass  # åœ¨éªŒè¯æ—¶è°ƒç”¨
        else:
            self.scheduler.step()

        avg_loss = total_loss / num_batches if num_batches > 0 else 0.0

        return {
            'loss': avg_loss,
            'learning_rate': self.optimizer.param_groups[0]['lr']
        }

    def validate(self, val_loader: DataLoader) -> Dict[str, float]:
        """éªŒè¯æ¨¡å‹"""
        self.model.eval()
        self.metrics.reset()

        # æ¸…ç†ä¹‹å‰çš„å€™é€‰æ± é‡Šæ”¾å†…å­˜
        if hasattr(self, '_candidate_pool'):
            del self._candidate_pool
        torch.cuda.empty_cache()  # æ¸…ç†GPUç¼“å­˜

        total_loss = 0.0
        num_batches = 0

        with torch.no_grad():
            progress_bar = tqdm(val_loader, desc="éªŒè¯")

            for batch in progress_bar:
                # æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡
                images = batch['images'].to(self.device)
                brep_graphs = batch['brep_graph'].to(self.device)
                labels = batch['labels'].to(self.device)

                batch_size = images.size(0)

                # æ„é€ å€™é€‰é›†è¿›è¡ŒéªŒè¯ï¼ˆé™åˆ¶å¤§å°é¿å…OOMï¼‰
                max_candidate_batches = 20  # å‡å°‘å€™é€‰æ± å¤§å°
                if not hasattr(self, '_candidate_pool') or len(self._candidate_pool['images']) < max_candidate_batches:
                    if not hasattr(self, '_candidate_pool'):
                        self._candidate_pool = {
                            'images': [],
                            'brep_graphs': [],
                            'labels': []
                        }

                    # é™åˆ¶å€™é€‰æ± å¤§å°ï¼Œé‡‡ç”¨FIFOç­–ç•¥
                    if len(self._candidate_pool['images']) >= max_candidate_batches:
                        # ç§»é™¤æœ€æ—§çš„æ‰¹æ¬¡
                        self._candidate_pool['images'].pop(0)
                        self._candidate_pool['brep_graphs'].pop(0)
                        self._candidate_pool['labels'].pop(0)

                    self._candidate_pool['images'].append(images.cpu())  # ç§»åˆ°CPUèŠ‚çœGPUå†…å­˜
                    self._candidate_pool['brep_graphs'].append(brep_graphs.cpu())
                    self._candidate_pool['labels'].append(labels.cpu())

                    # å¦‚æœå€™é€‰æ± è¿˜ä¸å¤Ÿå¤§ï¼Œè·³è¿‡è¿™ä¸ªæ‰¹æ¬¡çš„è¯„ä¼°
                    if len(self._candidate_pool['images']) < 3:  # å‡å°‘æœ€å°è¦æ±‚
                        continue

                # ä½¿ç”¨å€™é€‰æ± è¿›è¡Œè¯„ä¼°ï¼ˆé™åˆ¶æ•°é‡é¿å…OOMï¼‰
                max_candidates = min(5, len(self._candidate_pool['images']))  # å‡å°‘å€™é€‰æ•°é‡

                # ç§»å›GPUè¿›è¡Œè®¡ç®—
                candidate_images = torch.cat([img.to(self.device) for img in self._candidate_pool['images'][:max_candidates]], dim=0)

                # ä¼˜åŒ–å›¾å¤„ç†ï¼Œå‡å°‘å†…å­˜å ç”¨
                candidate_graph_list = self._candidate_pool['brep_graphs'][:max_candidates]
                normalized_graphs = []
                for graph in candidate_graph_list:
                    # ç§»åˆ°GPUå¹¶æ¸…ç†ç‰¹å¾
                    g = graph.to(self.device)
                    # åŸåœ°ç§»é™¤ç‰¹å¾ï¼Œé¿å…å…‹éš†
                    if 'edge_agg' in g.ndata:
                        del g.ndata['edge_agg']
                    if 'encoded' in g.edata:
                        del g.edata['encoded']
                    normalized_graphs.append(g)

                candidate_brep = dgl.batch(normalized_graphs)
                candidate_labels = torch.cat([lbl.to(self.device) for lbl in self._candidate_pool['labels'][:max_candidates]], dim=0)

                # æ‰§è¡Œæ£€ç´¢
                retrieval_results = self.model.retrieve(
                    images, brep_graphs,
                    candidate_images, candidate_brep,
                    k=10, modality='fused'
                )

                # æ›´æ–°è¯„ä¼°æŒ‡æ ‡
                self.metrics.update(
                    retrieval_results['indices'],
                    candidate_labels,
                    labels
                )

                num_batches += 1

        # è®¡ç®—æœ€ç»ˆæŒ‡æ ‡
        final_metrics = self.metrics.compute()

        # éªŒè¯ç»“æŸåæ¸…ç†å†…å­˜
        if hasattr(self, '_candidate_pool'):
            del self._candidate_pool
        torch.cuda.empty_cache()

        # å­¦ä¹ ç‡è°ƒåº¦ï¼ˆå¦‚æœä½¿ç”¨ReduceLROnPlateauï¼‰
        if isinstance(self.scheduler, optim.lr_scheduler.ReduceLROnPlateau):
            metric_for_scheduler = final_metrics.get('mAP', 0.0)
            self.scheduler.step(metric_for_scheduler)

        return final_metrics

    def save_checkpoint(self, filepath: str, epoch: int,
                       best_metric: float, is_best: bool = False):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'best_metric': best_metric,
            'model_config': self.model_config,
            'base_config': self.base_config
        }

        torch.save(checkpoint, filepath)

        if is_best:
            best_filepath = filepath.replace('.pth', '_best.pth')
            torch.save(checkpoint, best_filepath)

    def load_checkpoint(self, filepath: str) -> Tuple[int, float]:
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        checkpoint = torch.load(filepath, map_location=self.device)

        # è·å–å½“å‰æ¨¡å‹çš„çŠ¶æ€å­—å…¸
        current_state = self.model.state_dict()
        checkpoint_state = checkpoint['model_state_dict']

        # è¿‡æ»¤å‡ºåŒ¹é…çš„å‚æ•°
        filtered_state = {}
        missing_keys = []
        unexpected_keys = []

        for key, param in checkpoint_state.items():
            if key in current_state:
                if current_state[key].shape == param.shape:
                    filtered_state[key] = param
                else:
                    self.logger.warning(f"è·³è¿‡å½¢çŠ¶ä¸åŒ¹é…çš„å‚æ•°: {key} "
                                      f"(æ£€æŸ¥ç‚¹: {param.shape}, å½“å‰: {current_state[key].shape})")
                    missing_keys.append(key)
            else:
                unexpected_keys.append(key)

        # æ£€æŸ¥ç¼ºå¤±çš„é”®
        for key in current_state:
            if key not in filtered_state:
                missing_keys.append(key)

        # æ£€æŸ¥åŠ è½½æ¯”ä¾‹ï¼Œå¦‚æœå¤ªå°‘åˆ™è­¦å‘Š
        load_ratio = len(filtered_state) / len(current_state) if len(current_state) > 0 else 0

        if load_ratio < 0.5:
            self.logger.warning(f"æ£€æŸ¥ç‚¹å…¼å®¹æ€§è¾ƒä½ ({load_ratio:.1%})ï¼Œå¯èƒ½å½±å“æ¨¡å‹æ€§èƒ½")
            self.logger.warning("å»ºè®®åˆ é™¤æˆ–æ›´æ–°æ£€æŸ¥ç‚¹æ–‡ä»¶ï¼Œæˆ–ä»å¤´å¼€å§‹è®­ç»ƒ")

        # åŠ è½½è¿‡æ»¤åçš„çŠ¶æ€å­—å…¸
        self.model.load_state_dict(filtered_state, strict=False)

        # æ‰“å°åŠ è½½ä¿¡æ¯
        if missing_keys:
            self.logger.warning(f"ç¼ºå¤±çš„å‚æ•°: {missing_keys[:5]}{'...' if len(missing_keys) > 5 else ''}")
        if unexpected_keys:
            self.logger.warning(f"æ„å¤–çš„å‚æ•°: {unexpected_keys[:5]}{'...' if len(unexpected_keys) > 5 else ''}")

        self.logger.info(f"æˆåŠŸåŠ è½½ {len(filtered_state)}/{len(current_state)} ä¸ªå‚æ•° ({load_ratio:.1%})")

        # å°è¯•åŠ è½½ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨çŠ¶æ€ï¼ˆå¦‚æœå­˜åœ¨ä¸”å…¼å®¹ï¼‰
        try:
            if 'optimizer_state_dict' in checkpoint:
                self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        except Exception as e:
            self.logger.warning(f"æ— æ³•åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€: {e}")

        try:
            if 'scheduler_state_dict' in checkpoint:
                self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        except Exception as e:
            self.logger.warning(f"æ— æ³•åŠ è½½è°ƒåº¦å™¨çŠ¶æ€: {e}")

        return checkpoint.get('epoch', 0), checkpoint.get('best_metric', 0.0)

    def load_classification_checkpoint(self, filepath: str) -> None:
        """ä»åˆ†ç±»æ¨¡å‹æ£€æŸ¥ç‚¹åŠ è½½å…¼å®¹çš„ç»„ä»¶"""
        self.logger.info(f"ä»åˆ†ç±»æ£€æŸ¥ç‚¹åŠ è½½å…¼å®¹ç»„ä»¶: {filepath}")

        checkpoint = torch.load(filepath, map_location=self.device)
        classification_state = checkpoint['model_state_dict']

        # è·å–å½“å‰æ£€ç´¢æ¨¡å‹çš„çŠ¶æ€å­—å…¸
        retrieval_state = self.model.state_dict()

        # æ˜ å°„åˆ†ç±»æ¨¡å‹ç»„ä»¶åˆ°æ£€ç´¢æ¨¡å‹
        component_mapping = {
            # å›¾åƒç¼–ç å™¨
            'image_encoder': 'image_encoder',
            # Brepç¼–ç å™¨
            'brep_encoder': 'brep_encoder',
            # èåˆæ¨¡å—ï¼šåˆ†ç±»æ¨¡å‹ä¸­å« fuserï¼Œæ£€ç´¢æ¨¡å‹ä¸­å« fusion_module
            'fuser': 'fusion_module'
        }

        # åŠ è½½å…¼å®¹çš„ç»„ä»¶
        loaded_components = []
        for class_component, retrieval_component in component_mapping.items():
            component_loaded = False
            for key, value in classification_state.items():
                if key.startswith(f"{class_component}."):
                    # å°†åˆ†ç±»æ¨¡å‹çš„é”®æ˜ å°„åˆ°æ£€ç´¢æ¨¡å‹çš„é”®
                    new_key = key.replace(f"{class_component}.", f"{retrieval_component}.")
                    if new_key in retrieval_state:
                        retrieval_state[new_key] = value
                        component_loaded = True

            if component_loaded:
                loaded_components.append(retrieval_component)

        # åŠ è½½æ›´æ–°åçš„çŠ¶æ€å­—å…¸
        self.model.load_state_dict(retrieval_state, strict=False)

        self.logger.info(f"æˆåŠŸåŠ è½½ç»„ä»¶: {', '.join(loaded_components)}")
        self.logger.info("æ£€ç´¢ä¸“ç”¨ç»„ä»¶å°†ä»éšæœºåˆå§‹åŒ–å¼€å§‹è®­ç»ƒ")


def create_retrieval_pipeline(model_config: ModelConfig) -> MultiModalRetrievalPipeline:
    """åˆ›å»ºæ£€ç´¢æµæ°´çº¿çš„å·¥å‚å‡½æ•°"""
    return MultiModalRetrievalPipeline(model_config)


if __name__ == "__main__":
    # æµ‹è¯•æ£€ç´¢æµæ°´çº¿
    import dgl

    print("ğŸ§ª æµ‹è¯•æ£€ç´¢æµæ°´çº¿...")

    model_config = ModelConfig()
    base_config = BaseConfig()

    # åˆ›å»ºæµæ°´çº¿
    pipeline = create_retrieval_pipeline(model_config)

    # è·å–è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    pipeline.to(device)

    print(f"  ä½¿ç”¨è®¾å¤‡: {device}")

    # åˆ›å»ºæ¨¡æ‹ŸDGLå›¾
    def create_mock_graph(num_nodes=8):
        src = torch.tensor([0, 1, 2, 3, 4, 5, 6, 7, 0, 1, 2])
        dst = torch.tensor([1, 2, 3, 4, 5, 6, 7, 0, 2, 3, 4])
        graph = dgl.graph((src, dst), num_nodes=num_nodes)
        graph = dgl.add_self_loop(graph)
        graph.ndata['x'] = torch.randn(num_nodes, 7)
        return graph

    # æ¨¡æ‹Ÿæ•°æ®
    batch_size = 4  # å‡å°æ‰¹æ¬¡å¤§å°
    num_views = 3
    height, width = 224, 224

    # åˆ›å»ºå›¾åƒæ•°æ®
    images = torch.randn(batch_size, num_views, 3, height, width).to(device)

    # åˆ›å»ºDGLå›¾æ•°æ®
    graphs = []
    for _ in range(batch_size):
        graphs.append(create_mock_graph())
    brep_graphs = dgl.batch(graphs).to(device)

    print(f"  å›¾åƒæ•°æ®å½¢çŠ¶: {images.shape}")
    print(f"  å›¾æ•°æ®: {brep_graphs.batch_size} ä¸ªå›¾")

    # æµ‹è¯•ç‰¹å¾ç¼–ç 
    with torch.no_grad():
        print("  1. æµ‹è¯•ç‰¹å¾ç¼–ç ...")

        # åˆ›å»ºæŸ¥è¯¢å›¾ï¼ˆå‰2ä¸ªï¼‰
        query_graphs = []
        for i in range(2):
            query_graphs.append(create_mock_graph())
        query_brep_graphs = dgl.batch(query_graphs).to(device)

        query_features = pipeline.encode_features(images[:2], query_brep_graphs)
        candidate_features = pipeline.encode_features(images, brep_graphs)

        print(f"     æŸ¥è¯¢ç‰¹å¾å½¢çŠ¶: {query_features['fused_features'].shape}")
        print(f"     å€™é€‰ç‰¹å¾å½¢çŠ¶: {candidate_features['fused_features'].shape}")

        # æµ‹è¯•æ£€ç´¢åŠŸèƒ½
        print("  2. æµ‹è¯•æ£€ç´¢åŠŸèƒ½...")
        retrieval_results = pipeline.retrieve(
            images[:2], query_brep_graphs,
            images, brep_graphs,
            k=3, modality='fused'
        )

        print(f"     æ£€ç´¢ç»“æœå½¢çŠ¶: {retrieval_results['indices'].shape}")
        print(f"     æ£€ç´¢ç›¸ä¼¼åº¦å½¢çŠ¶: {retrieval_results['similarities'].shape}")
        print(f"     Top-3ç´¢å¼•: {retrieval_results['indices'][0].tolist()}")

    print("âœ“ æ£€ç´¢æµæ°´çº¿æµ‹è¯•é€šè¿‡ï¼")