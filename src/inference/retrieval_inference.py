#!/usr/bin/env python3
"""
CADNETæ£€ç´¢æ¨¡å‹æ¨ç†è„šæœ¬ - æ”¯æŒå¤šæ¨¡æ€ç›¸ä¼¼æ€§æ£€ç´¢
"""

import os
import sys
import argparse
import torch
import numpy as np
from PIL import Image
import dgl
import json
from pathlib import Path
from typing import List, Dict, Optional, Tuple

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(__file__)))

from config.model_config import ModelConfig
from src.models.downstream.retrieval_pipeline import create_retrieval_pipeline
from src.data.multimodal_dataset import MultiModalCADDataset


def load_retrieval_model(model_path: str):
    """åŠ è½½æ£€ç´¢æ¨¡å‹"""
    print(f"åŠ è½½æ£€ç´¢æ¨¡å‹: {model_path}")

    if not os.path.exists(model_path):
        raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")

    checkpoint = torch.load(model_path, map_location='cpu')

    # è·å–é…ç½®
    if 'model_config' in checkpoint:
        config = checkpoint['model_config']
    elif 'config' in checkpoint:
        config = checkpoint['config']
    else:
        print("è­¦å‘Š: ä½¿ç”¨é»˜è®¤é…ç½®")
        config = ModelConfig()

    # åˆ›å»ºæ¨¡å‹
    model = create_retrieval_pipeline(config)

    # åŠ è½½çŠ¶æ€å­—å…¸
    state_dict = checkpoint.get('model_state_dict', checkpoint)

    try:
        model.load_state_dict(state_dict, strict=True)
        print("âœ“ æ¨¡å‹çŠ¶æ€å­—å…¸åŠ è½½æˆåŠŸ")
    except RuntimeError as e:
        print(f"ä¸¥æ ¼æ¨¡å¼åŠ è½½å¤±è´¥: {e}")
        missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)
        if missing_keys:
            print(f"ç¼ºå¤±çš„é”®: {missing_keys[:3]}{'...' if len(missing_keys) > 3 else ''}")
        if unexpected_keys:
            print(f"æ„å¤–çš„é”®: {unexpected_keys[:3]}{'...' if len(unexpected_keys) > 3 else ''}")
        print("âœ“ æ¨¡å‹çŠ¶æ€å­—å…¸åŠ è½½æˆåŠŸï¼ˆéä¸¥æ ¼æ¨¡å¼ï¼‰")

    model.eval()

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device)

    print(f"æ£€ç´¢æ¨¡å‹åŠ è½½å®Œæˆï¼Œè®¾å¤‡: {device}")
    return model, device, config


def preprocess_images(image_paths: List[str], target_size=(224, 224)):
    """é¢„å¤„ç†å›¾åƒæ•°æ®"""
    images = []
    valid_count = 0

    # ç¡®ä¿æœ‰3ä¸ªè§†å›¾ï¼ˆä¸è¶³çš„ç”¨ç©ºç™½å¡«å……ï¼‰
    while len(image_paths) < 3:
        image_paths.append(None)

    for i, path in enumerate(image_paths[:3]):  # åªå–å‰3ä¸ª
        if path and os.path.exists(path):
            try:
                image = Image.open(path).convert('RGB')
                image = image.resize(target_size, Image.LANCZOS)

                # è½¬æ¢ä¸ºtensorå¹¶å½’ä¸€åŒ–
                image_array = np.array(image).astype(np.float32) / 255.0

                # æ ‡å‡†åŒ– (ImageNetæ ‡å‡†)
                mean = np.array([0.485, 0.456, 0.406])
                std = np.array([0.229, 0.224, 0.225])
                image_array = (image_array - mean) / std

                image_tensor = torch.from_numpy(image_array).permute(2, 0, 1)
                images.append(image_tensor)
                valid_count += 1
                print(f"âœ“ åŠ è½½å›¾åƒ {i+1}: {os.path.basename(path)}")
            except Exception as e:
                print(f"âœ— å›¾åƒåŠ è½½å¤±è´¥ {i+1}: {e}")
                blank_image = torch.zeros(3, target_size[0], target_size[1])
                images.append(blank_image)
        else:
            print(f"âœ— å›¾åƒè·¯å¾„ä¸ºç©ºæˆ–ä¸å­˜åœ¨ {i+1}")
            blank_image = torch.zeros(3, target_size[0], target_size[1])
            images.append(blank_image)

    if valid_count == 0:
        return None

    print(f"âœ“ æˆåŠŸå¤„ç† {valid_count}/3 ä¸ªå›¾åƒ")
    return torch.stack(images).unsqueeze(0)  # [1, 3, C, H, W]


def load_brep_graph(brep_path: str):
    """åŠ è½½Brepå›¾æ•°æ®"""
    if not brep_path or not os.path.exists(brep_path):
        return None

    try:
        graphs, _ = dgl.data.utils.load_graphs(brep_path)
        graph = graphs[0]

        # ç¡®ä¿ç‰¹å¾ç±»å‹æ­£ç¡®
        if 'x' in graph.ndata:
            graph.ndata['x'] = graph.ndata['x'].float()
        if 'x' in graph.edata:
            graph.edata['x'] = graph.edata['x'].float()

        print(f"âœ“ åŠ è½½Brepå›¾: {os.path.basename(brep_path)}")
        print(f"  èŠ‚ç‚¹æ•°: {graph.num_nodes()}, è¾¹æ•°: {graph.num_edges()}")
        return graph

    except Exception as e:
        print(f"âœ— Brepå›¾åŠ è½½å¤±è´¥: {e}")
        return None


def build_candidate_database(model, device, annotation_file: str, max_candidates: int = 1000):
    """æ„å»ºå€™é€‰æ•°æ®åº“"""
    print(f"\nğŸ“š æ„å»ºå€™é€‰æ•°æ®åº“...")

    with open(annotation_file, 'r', encoding='utf-8') as f:
        annotations = json.load(f)

    # è·å–æ‰€æœ‰æ¨¡å‹ID
    model_ids = [k for k in annotations.keys() if not k.startswith('_')]
    if len(model_ids) > max_candidates:
        import random
        random.seed(42)
        model_ids = random.sample(model_ids, max_candidates)

    print(f"å¤„ç† {len(model_ids)} ä¸ªå€™é€‰æ ·æœ¬...")

    candidate_features = []
    candidate_metadata = []
    processed_count = 0

    for i, model_id in enumerate(model_ids):
        if i % 100 == 0:
            print(f"  è¿›åº¦: {i}/{len(model_ids)}")

        annotation = annotations[model_id]
        available_data = annotation.get('available_data', {})

        # è·å–å›¾åƒè·¯å¾„
        views = available_data.get('views', {})
        image_paths = [
            views.get('front', ''),
            views.get('side', ''),
            views.get('top', '')
        ]

        # è·å–Brepè·¯å¾„
        brep_path = available_data.get('brep', '')

        # é¢„å¤„ç†è¾“å…¥
        images = preprocess_images(image_paths) if any(image_paths) else None
        brep_graph = load_brep_graph(brep_path) if brep_path else None

        if images is None and brep_graph is None:
            continue

        try:
            # ç¼–ç ç‰¹å¾
            with torch.no_grad():
                if images is not None:
                    images = images.to(device)
                if brep_graph is not None:
                    brep_graph = dgl.batch([brep_graph]).to(device)

                features = model.encode_features(
                    images if images is not None else torch.zeros(1, 3, 3, 224, 224).to(device),
                    brep_graph if brep_graph is not None else None
                )

                candidate_features.append({
                    'fused_features': features['fused_features'].cpu(),
                    'image_enhanced': features['image_enhanced'].cpu(),
                    'brep_enhanced': features['brep_enhanced'].cpu()
                })

                candidate_metadata.append({
                    'model_id': model_id,
                    'category': annotation.get('metadata', {}).get('category', 'unknown'),
                    'has_images': images is not None,
                    'has_brep': brep_graph is not None
                })

                processed_count += 1

        except Exception as e:
            print(f"  è·³è¿‡ {model_id}: {e}")
            continue

    print(f"âœ“ æˆåŠŸæ„å»ºå€™é€‰æ•°æ®åº“: {processed_count} ä¸ªæ ·æœ¬")

    # è½¬æ¢ä¸ºæ‰¹é‡å¼ é‡
    if candidate_features:
        candidate_batch = {
            'fused_features': torch.cat([f['fused_features'] for f in candidate_features], dim=0),
            'image_enhanced': torch.cat([f['image_enhanced'] for f in candidate_features], dim=0),
            'brep_enhanced': torch.cat([f['brep_enhanced'] for f in candidate_features], dim=0)
        }
    else:
        candidate_batch = None

    return candidate_batch, candidate_metadata


def retrieve_similar(model, device, query_images=None, query_brep=None,
                    candidate_features=None, candidate_metadata=None,
                    k: int = 10, modality: str = 'fused'):
    """æ‰§è¡Œç›¸ä¼¼æ€§æ£€ç´¢"""
    print(f"\nğŸ” æ‰§è¡Œæ£€ç´¢...")
    print(f"  æ£€ç´¢æ¨¡æ€: {modality}")
    print(f"  è¿”å›æ•°é‡: Top-{k}")

    if candidate_features is None or not candidate_metadata:
        print("âŒ å€™é€‰æ•°æ®åº“ä¸ºç©º")
        return None

    # ç¼–ç æŸ¥è¯¢ç‰¹å¾
    with torch.no_grad():
        if query_images is not None:
            query_images = query_images.to(device)
        if query_brep is not None:
            query_brep = dgl.batch([query_brep]).to(device)

        query_features = model.encode_features(
            query_images if query_images is not None else torch.zeros(1, 3, 3, 224, 224).to(device),
            query_brep if query_brep is not None else None
        )

        # ç§»åŠ¨å€™é€‰ç‰¹å¾åˆ°è®¾å¤‡
        candidate_batch = {
            key: features.to(device)
            for key, features in candidate_features.items()
        }

        # æ‰§è¡Œæ£€ç´¢
        results = model.retrieval_module.retrieve(
            query_features, candidate_batch, k=k, modality=modality
        )

    # å¤„ç†ç»“æœ
    similarities = results['similarities'].cpu().numpy().flatten()
    indices = results['indices'].cpu().numpy().flatten()

    retrieved_results = []
    for i, (idx, sim) in enumerate(zip(indices, similarities)):
        if idx < len(candidate_metadata):
            metadata = candidate_metadata[idx]
            retrieved_results.append({
                'rank': i + 1,
                'model_id': metadata['model_id'],
                'category': metadata['category'],
                'similarity': float(sim),
                'has_images': metadata['has_images'],
                'has_brep': metadata['has_brep']
            })

    return retrieved_results


def retrieve_from_query(model, device, query_images=None, query_brep=None,
                       candidate_features=None, candidate_metadata=None,
                       k: int = 10):
    """ä»æŸ¥è¯¢æ‰§è¡Œæ£€ç´¢"""
    if query_images is None and query_brep is None:
        print("âŒ è‡³å°‘éœ€è¦æä¾›ä¸€ç§æŸ¥è¯¢æ¨¡æ€")
        return None

    # ç¡®å®šæŸ¥è¯¢æ¨¡æ€
    if query_images is not None and query_brep is not None:
        query_modality = "dual"
        modalities = ['fused', 'image', 'brep']
    elif query_images is not None:
        query_modality = "image"
        modalities = ['image', 'fused']
    else:
        query_modality = "brep"
        modalities = ['brep', 'fused']

    print(f"\nğŸ¯ æŸ¥è¯¢æ¨¡æ€: {query_modality}")

    all_results = {}
    for modality in modalities:
        print(f"\n--- ä½¿ç”¨ {modality} ç‰¹å¾æ£€ç´¢ ---")
        results = retrieve_similar(
            model, device, query_images, query_brep,
            candidate_features, candidate_metadata, k, modality
        )
        if results:
            all_results[modality] = results

    return all_results


def display_retrieval_results(results: Dict[str, List[Dict]], top_n: int = 5):
    """æ˜¾ç¤ºæ£€ç´¢ç»“æœ"""
    print(f"\nğŸ“Š æ£€ç´¢ç»“æœ (Top-{top_n}):")
    print("=" * 80)

    for modality, result_list in results.items():
        print(f"\nğŸ”¹ {modality.upper()} ç‰¹å¾æ£€ç´¢:")
        print("-" * 50)

        for i, result in enumerate(result_list[:top_n]):
            print(f"  {result['rank']:2d}. {result['model_id']:<15} "
                  f"ç±»åˆ«: {result['category']:<12} "
                  f"ç›¸ä¼¼åº¦: {result['similarity']:.4f} "
                  f"{'ğŸ“·' if result['has_images'] else 'âŒ'}"
                  f"{'ğŸ”§' if result['has_brep'] else 'âŒ'}")


def retrieve_from_annotation(model, device, annotation_file: str, query_id: str,
                            candidate_features=None, candidate_metadata=None, k: int = 10):
    """ä»æ ‡æ³¨æ–‡ä»¶æ£€ç´¢"""
    with open(annotation_file, 'r', encoding='utf-8') as f:
        annotations = json.load(f)

    if query_id not in annotations:
        print(f"âŒ æŸ¥è¯¢ID {query_id} ä¸å­˜åœ¨")
        return None

    query_annotation = annotations[query_id]
    available_data = query_annotation.get('available_data', {})

    print(f"\nğŸ” æ£€ç´¢æŸ¥è¯¢: {query_id}")
    print(f"  çœŸå®ç±»åˆ«: {query_annotation.get('metadata', {}).get('category', 'unknown')}")

    # è·å–æŸ¥è¯¢æ•°æ®
    views = available_data.get('views', {})
    image_paths = [views.get('front', ''), views.get('side', ''), views.get('top', '')]
    brep_path = available_data.get('brep', '')

    # é¢„å¤„ç†æŸ¥è¯¢è¾“å…¥
    query_images = preprocess_images(image_paths) if any(image_paths) else None
    query_brep = load_brep_graph(brep_path) if brep_path else None

    if query_images is None and query_brep is None:
        print("âŒ æŸ¥è¯¢æ ·æœ¬æ²¡æœ‰æœ‰æ•ˆæ•°æ®")
        return None

    # æ‰§è¡Œæ£€ç´¢
    results = retrieve_from_query(
        model, device, query_images, query_brep,
        candidate_features, candidate_metadata, k
    )

    if results:
        display_retrieval_results(results, top_n=k)

        # è®¡ç®—æ£€ç´¢å‡†ç¡®ç‡
        true_category = query_annotation.get('metadata', {}).get('category', 'unknown')
        print(f"\nğŸ“ˆ æ£€ç´¢å‡†ç¡®ç‡åˆ†æ:")

        for modality, result_list in results.items():
            correct_at_k = []
            for k_val in [1, 5, 10]:
                if k_val <= len(result_list):
                    top_k_categories = [r['category'] for r in result_list[:k_val]]
                    correct = true_category in top_k_categories
                    correct_at_k.append(f"R@{k_val}: {'âœ…' if correct else 'âŒ'}")

            print(f"  {modality}: {' | '.join(correct_at_k)}")

    return results


def main():
    parser = argparse.ArgumentParser(description="CADNETæ£€ç´¢æ¨¡å‹æ¨ç†")
    parser.add_argument(
        "--model_path",
        type=str,
        required=True,
        help="è®­ç»ƒå¥½çš„æ£€ç´¢æ¨¡å‹è·¯å¾„"
    )
    parser.add_argument(
        "--annotation_file",
        type=str,
        default="letters_annotations.json",
        help="æ ‡æ³¨æ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        "--query_id",
        type=str,
        help="æŸ¥è¯¢æ¨¡å‹IDï¼ˆä»æ ‡æ³¨æ–‡ä»¶ï¼‰"
    )
    parser.add_argument(
        "--query_images",
        nargs='*',
        help="æŸ¥è¯¢å›¾åƒè·¯å¾„åˆ—è¡¨"
    )
    parser.add_argument(
        "--query_brep",
        type=str,
        help="æŸ¥è¯¢Brepæ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        "--top_k",
        type=int,
        default=10,
        help="è¿”å›Top-Kç»“æœæ•°é‡"
    )
    parser.add_argument(
        "--max_candidates",
        type=int,
        default=1000,
        help="æœ€å¤§å€™é€‰æ ·æœ¬æ•°é‡"
    )
    parser.add_argument(
        "--modality",
        choices=['image', 'brep', 'fused', 'all'],
        default='all',
        help="æ£€ç´¢ä½¿ç”¨çš„ç‰¹å¾æ¨¡æ€"
    )

    args = parser.parse_args()

    # æ£€æŸ¥æ–‡ä»¶
    if not os.path.exists(args.model_path):
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {args.model_path}")
        return

    if not os.path.exists(args.annotation_file):
        print(f"âŒ æ ‡æ³¨æ–‡ä»¶ä¸å­˜åœ¨: {args.annotation_file}")
        return

    # åŠ è½½æ¨¡å‹
    try:
        model, device, config = load_retrieval_model(args.model_path)
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return

    # æ„å»ºå€™é€‰æ•°æ®åº“
    try:
        candidate_features, candidate_metadata = build_candidate_database(
            model, device, args.annotation_file, args.max_candidates
        )
        if candidate_features is None:
            print("âŒ å€™é€‰æ•°æ®åº“æ„å»ºå¤±è´¥")
            return
    except Exception as e:
        print(f"âŒ å€™é€‰æ•°æ®åº“æ„å»ºå¤±è´¥: {e}")
        return

    # æ‰§è¡Œæ£€ç´¢
    if args.query_id:
        # ä»æ ‡æ³¨æ–‡ä»¶æ£€ç´¢
        retrieve_from_annotation(
            model, device, args.annotation_file, args.query_id,
            candidate_features, candidate_metadata, args.top_k
        )

    elif args.query_images or args.query_brep:
        # ç›´æ¥æ–‡ä»¶è·¯å¾„æ£€ç´¢
        query_images = preprocess_images(args.query_images) if args.query_images else None
        query_brep = load_brep_graph(args.query_brep) if args.query_brep else None

        if query_images is None and query_brep is None:
            print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„æŸ¥è¯¢è¾“å…¥")
            return

        results = retrieve_from_query(
            model, device, query_images, query_brep,
            candidate_features, candidate_metadata, args.top_k
        )

        if results:
            display_retrieval_results(results, args.top_k)

    else:
        print("âŒ è¯·æŒ‡å®šæŸ¥è¯¢æ–¹å¼:")
        print("  --query_id MODEL_ID                    # ä»æ ‡æ³¨æ–‡ä»¶æ£€ç´¢")
        print("  --query_images path1 [path2 path3]     # å›¾åƒæŸ¥è¯¢")
        print("  --query_brep PATH                      # BrepæŸ¥è¯¢")
        print("  --query_images ... --query_brep ...    # å¤šæ¨¡æ€æŸ¥è¯¢")
        print("")
        print("ğŸ“ ä½¿ç”¨ç¤ºä¾‹:")
        print("  # ä»æ ‡æ³¨æ–‡ä»¶æ£€ç´¢")
        print("  python retrieval_inference.py --model_path model.pth --query_id MODEL_001")
        print("")
        print("  # å›¾åƒæ£€ç´¢")
        print("  python retrieval_inference.py --model_path model.pth --query_images front.jpg side.jpg")
        print("")
        print("  # Brepæ£€ç´¢")
        print("  python retrieval_inference.py --model_path model.pth --query_brep query.dgl")


if __name__ == "__main__":
    main()